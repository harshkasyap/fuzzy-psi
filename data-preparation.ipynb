{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82547fa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e13a6861",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b2/kl7kvjxj3_93xk76yljmbmdw0000gr/T/ipykernel_87263/1359130341.py:4: DtypeWarning: Columns (25,27,28,29,47,49,51,55,57,63,65,67,69,71,73,75,77,79,81,87) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  vr_snapshot_df1 = pd.read_csv(file_path1, delimiter='\\t', encoding='utf-16le')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File read successfully.\n"
     ]
    }
   ],
   "source": [
    "file_path1 = 'VR_Snapshot_20140101.txt' \n",
    "\n",
    "try:\n",
    "    vr_snapshot_df1 = pd.read_csv(file_path1, delimiter='\\t', encoding='utf-16le')\n",
    "    print(\"File read successfully.\")\n",
    "    #print(vr_snapshot_df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found at {file_path1}. Please check the file path.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "496d1b53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b2/kl7kvjxj3_93xk76yljmbmdw0000gr/T/ipykernel_87263/1684270408.py:4: DtypeWarning: Columns (25,27,28,29,47,49,51,55,57,63,65,67,69,71,73,75,77,79,81,87) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  vr_snapshot_df2 = pd.read_csv(file_path2, delimiter='\\t', encoding='utf-16le')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File read successfully.\n"
     ]
    }
   ],
   "source": [
    "file_path2 = 'VR_Snapshot_20170101.txt' \n",
    "\n",
    "try:\n",
    "    vr_snapshot_df2 = pd.read_csv(file_path2, delimiter='\\t', encoding='utf-16le')\n",
    "    print(\"File read successfully.\")\n",
    "    #print(vr_snapshot_df.head())\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found at {file_path2}. Please check the file path.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while reading the file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "1c9063c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_select = ['voter_reg_num', 'ncid', 'last_name', 'first_name', 'midl_name', 'name_sufx_cd', 'age']\n",
    "selected_df1 = vr_snapshot_df1[columns_to_select]\n",
    "selected_df2 = vr_snapshot_df2[columns_to_select]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1ac21bdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4273b52b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_ncids = selected_df1.drop_duplicates(subset='ncid')\n",
    "sorted_unique_df = df_unique_ncids.sort_values(by='ncid', ascending=True)\n",
    "selected_unique_df1 = sorted_unique_df.head(10000)\n",
    "duplicates_check = selected_unique_df1[selected_unique_df1.duplicated('ncid', keep=False)]\n",
    "duplicates_found = not duplicates_check.empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "7932543f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_unique_ncids2 = selected_df2.drop_duplicates(subset='ncid')\n",
    "sorted_unique_df2 = df_unique_ncids2.sort_values(by='ncid', ascending=True)\n",
    "selected_unique_df2 = sorted_unique_df2.head(10000)\n",
    "duplicates_check2 = selected_unique_df2[selected_unique_df2.duplicated('ncid', keep=False)]\n",
    "duplicates_found2 = not duplicates_check2.empty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a63703a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate NCID values found.\n"
     ]
    }
   ],
   "source": [
    "duplicate_ncids = selected_unique_df1[selected_unique_df1.duplicated('ncid', keep=False)]\n",
    "if not duplicate_ncids.empty:\n",
    "    print(\"Duplicate NCID values found:\")\n",
    "    print(duplicate_ncids)\n",
    "else:\n",
    "    print(\"No duplicate NCID values found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ccca4a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No duplicate NCID values found.\n"
     ]
    }
   ],
   "source": [
    "duplicate_ncids2 = selected_unique_df2[selected_unique_df2.duplicated('ncid', keep=False)]\n",
    "if not duplicate_ncids.empty:\n",
    "    print(\"Duplicate NCID values found:\")\n",
    "    print(duplicate_ncids2)\n",
    "else:\n",
    "    print(\"No duplicate NCID values found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1b51f55f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_unique_ids = selected_unique_df1['ncid'].nunique()\n",
    "num_unique_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eea58a06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10000"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_unique_ids = selected_unique_df2['ncid'].nunique()\n",
    "num_unique_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d83e915a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9985"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids_in_both_datasets = selected_unique_df1['ncid'][selected_unique_df1['ncid'].isin(selected_unique_df2['ncid'])]\n",
    "num_ids_in_both = ids_in_both_datasets.nunique()\n",
    "num_ids_in_both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4e3cb716",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b2/kl7kvjxj3_93xk76yljmbmdw0000gr/T/ipykernel_87263/1397806001.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  selected_unique_df1['Date of Birth'] = 2014 - selected_unique_df1['age']\n"
     ]
    }
   ],
   "source": [
    "if 'age' in selected_unique_df1.columns:\n",
    "    # Calculate the birth year and create a new column \"Date of Birth\"\n",
    "    selected_unique_df1['Date of Birth'] = 2014 - selected_unique_df1['age']\n",
    "else:\n",
    "    print(\"The 'age' column is not present in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e23d2f80",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b2/kl7kvjxj3_93xk76yljmbmdw0000gr/T/ipykernel_87263/37178658.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  selected_unique_df2['Date of Birth'] = 2018 - selected_unique_df2['age']\n"
     ]
    }
   ],
   "source": [
    "if 'age' in selected_unique_df2.columns:\n",
    "    # Calculate the birth year and create a new column \"Date of Birth\"\n",
    "    selected_unique_df2['Date of Birth'] = 2018 - selected_unique_df2['age']\n",
    "else:\n",
    "    print(\"The 'age' column is not present in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "e2344111",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/b2/kl7kvjxj3_93xk76yljmbmdw0000gr/T/ipykernel_87263/2941278902.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  selected_unique_df1['Full Name'] = selected_unique_df1.apply(create_full_name, axis=1)\n",
      "/var/folders/b2/kl7kvjxj3_93xk76yljmbmdw0000gr/T/ipykernel_87263/2941278902.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  selected_unique_df2['Full Name'] = selected_unique_df2.apply(create_full_name, axis=1)\n"
     ]
    }
   ],
   "source": [
    "def create_full_name(row):\n",
    "    # Convert each part to string, replace NaN with empty string\n",
    "    first_name = str(row['first_name']) if pd.notna(row['first_name']) else ''\n",
    "    midl_name = str(row['midl_name']) if pd.notna(row['midl_name']) else ''\n",
    "    last_name = str(row['last_name']) if pd.notna(row['last_name']) else ''\n",
    "    name_sufx_cd = str(row['name_sufx_cd']) if pd.notna(row['name_sufx_cd']) else ''\n",
    "\n",
    "    # Create the full name, skipping any empty parts\n",
    "    full_name = ' '.join(filter(None, [first_name, midl_name, last_name, name_sufx_cd]))\n",
    "    return full_name\n",
    "\n",
    "# Apply the function to each row\n",
    "selected_unique_df1['Full Name'] = selected_unique_df1.apply(create_full_name, axis=1)\n",
    "selected_unique_df2['Full Name'] = selected_unique_df2.apply(create_full_name, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "c36515b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_unique_df1 = selected_unique_df1[['ncid', 'Full Name', 'Date of Birth']]\n",
    "selected_unique_df2 = selected_unique_df2[['ncid', 'Full Name', 'Date of Birth']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "97bea9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=selected_unique_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8fbe08fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=selected_unique_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4f391cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a left merge to add the 'Full Name' from df1 to df2 based on 'ncid'\n",
    "df2 = df2.merge(df1[['ncid', 'Full Name']], on='ncid', how='left', suffixes=('', ' Original'))\n",
    "\n",
    "# Rename the merged 'Full Name' column to 'Original Full Name'\n",
    "df2.rename(columns={'Full Name Original': 'Original Full Name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "17d98ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.rename(columns={'ncid': 'ID', \n",
    "                    'Original Full Name': 'Full Name1', \n",
    "                    'Full Name': 'Fuzzy Full Name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "8230c20b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df2[['ID', 'Full Name1', 'Fuzzy Full Name', 'Date of Birth']]\n",
    "df2.rename(columns={'Full Name1': 'Full Name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "bc7c2c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.rename(columns={'ncid': 'ID'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a708759d",
   "metadata": {},
   "outputs": [],
   "source": [
    "del df2['Full Name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "0b540aed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Full Name</th>\n",
       "      <th>Date of Birth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>156142</th>\n",
       "      <td>AA10000</td>\n",
       "      <td>JAMES ALFRED CLUBB</td>\n",
       "      <td>1922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42559</th>\n",
       "      <td>AA100000</td>\n",
       "      <td>JAMES ARCHIE BOYDSTUN II</td>\n",
       "      <td>1968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42560</th>\n",
       "      <td>AA100001</td>\n",
       "      <td>LEWIS ANDREW BRANDON</td>\n",
       "      <td>1978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42561</th>\n",
       "      <td>AA100002</td>\n",
       "      <td>TRACEY ANN DRAHUSHUK</td>\n",
       "      <td>1970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42562</th>\n",
       "      <td>AA100003</td>\n",
       "      <td>RICHARD LEE BRIDGES</td>\n",
       "      <td>1965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39041</th>\n",
       "      <td>AA109125</td>\n",
       "      <td>MATTHEW BENJAMIN DODSON</td>\n",
       "      <td>1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39042</th>\n",
       "      <td>AA109126</td>\n",
       "      <td>RICHARD EARNEST GREEN III</td>\n",
       "      <td>1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39043</th>\n",
       "      <td>AA109127</td>\n",
       "      <td>AMANDA ALANA JENKINS</td>\n",
       "      <td>1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39044</th>\n",
       "      <td>AA109128</td>\n",
       "      <td>PATRICK MCKINLEY WYRICK</td>\n",
       "      <td>1978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39045</th>\n",
       "      <td>AA109129</td>\n",
       "      <td>SHARON ANN YELLOCK</td>\n",
       "      <td>1968</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID                    Full Name  Date of Birth\n",
       "156142   AA10000       JAMES ALFRED CLUBB               1922\n",
       "42559   AA100000    JAMES ARCHIE BOYDSTUN II            1968\n",
       "42560   AA100001     LEWIS ANDREW BRANDON               1978\n",
       "42561   AA100002     TRACEY ANN DRAHUSHUK               1970\n",
       "42562   AA100003      RICHARD LEE BRIDGES               1965\n",
       "...          ...                          ...            ...\n",
       "39041   AA109125  MATTHEW BENJAMIN DODSON               1980\n",
       "39042   AA109126    RICHARD EARNEST GREEN III           1969\n",
       "39043   AA109127     AMANDA ALANA JENKINS               1980\n",
       "39044   AA109128  PATRICK MCKINLEY WYRICK               1978\n",
       "39045   AA109129       SHARON ANN YELLOCK               1968\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c34f1431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Fuzzy Full Name</th>\n",
       "      <th>Date of Birth</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AA10000</td>\n",
       "      <td>JAMES ALFRED CLUBB</td>\n",
       "      <td>1923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AA100000</td>\n",
       "      <td>JAMES ARCHIE BOYDSTUN II</td>\n",
       "      <td>1969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AA100001</td>\n",
       "      <td>LEWIS ANDREW BRANDON</td>\n",
       "      <td>1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AA100002</td>\n",
       "      <td>TRACEY ANN DRAHUSHUK</td>\n",
       "      <td>1971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AA100003</td>\n",
       "      <td>RICHARD LEE BRIDGES</td>\n",
       "      <td>1966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>AA109139</td>\n",
       "      <td>SHEA BALDWIN HUFF</td>\n",
       "      <td>1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>AA10914</td>\n",
       "      <td>RALPH BOYD COOK</td>\n",
       "      <td>1915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>AA109140</td>\n",
       "      <td>JOHN LUCAS BARKER</td>\n",
       "      <td>1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>AA109141</td>\n",
       "      <td>RICHARD GLENN BARNES SR</td>\n",
       "      <td>1958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>AA109142</td>\n",
       "      <td>DANNY LEE BATEMAN JR</td>\n",
       "      <td>1982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID           Fuzzy Full Name  Date of Birth\n",
       "0      AA10000      JAMES ALFRED CLUBB             1923\n",
       "1     AA100000  JAMES ARCHIE BOYDSTUN II           1969\n",
       "2     AA100001    LEWIS ANDREW BRANDON             1979\n",
       "3     AA100002    TRACEY ANN DRAHUSHUK             1971\n",
       "4     AA100003     RICHARD LEE BRIDGES             1966\n",
       "...        ...                       ...            ...\n",
       "9995  AA109139       SHEA BALDWIN HUFF             1980\n",
       "9996   AA10914         RALPH BOYD COOK             1915\n",
       "9997  AA109140       JOHN LUCAS BARKER             1977\n",
       "9998  AA109141   RICHARD GLENN BARNES SR           1958\n",
       "9999  AA109142      DANNY LEE BATEMAN JR           1982\n",
       "\n",
       "[10000 rows x 3 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "9c21d7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from random import randint\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import jellyfish\n",
    "import itertools\n",
    "from sklearn.metrics import jaccard_score\n",
    "from nltk import ngrams\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import string\n",
    "import hashlib\n",
    "import math\n",
    "from numpy.linalg import norm\n",
    "import time\n",
    "import psutil\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "3127e1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1['Full Name'] = df1['Full Name'].str.lower()\n",
    "#df2['Full Name'] = df2['Full Name'].str.lower()\n",
    "df2['Fuzzy Full Name'] = df2['Fuzzy Full Name'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ee987012",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "shingle_size = 3\n",
    "num_permutations = 200\n",
    "num_permutations2 = 50\n",
    "num_permutations3 = 100\n",
    "max_hash = (2**20)-1 #(2 ** 31) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a84e178a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate the shingles of a string\n",
    "def generate_shingles(string, shingle_size):\n",
    "    shingles = set()\n",
    "    for i in range(len(string) - shingle_size + 1):\n",
    "        shingle = string[i:i + shingle_size]\n",
    "        shingles.add(shingle)\n",
    "    return shingles\n",
    "\n",
    "# Function to generate a hash value for a shingle\n",
    "def hash_shingle(shingle):\n",
    "    return int(hashlib.sha256(shingle.encode()).hexdigest(), 32)\n",
    "\n",
    "# Function to generate a random permutation function\n",
    "def generate_permutation_function(num_permutations, max_hash):\n",
    "    def permutation_function(x):\n",
    "        random.seed(x)\n",
    "        a = random.randint(1, max_hash)\n",
    "        b = random.randint(0, max_hash)\n",
    "        return lambda h: (a * h + b) % max_hash\n",
    "    return [permutation_function(i) for i in range(num_permutations)]\n",
    "\n",
    "# Function to compute the MinHash signature of a set of shingles\n",
    "def compute_minhash_signature(shingles, permutation_functions):\n",
    "    signature = [float('inf')] * len(permutation_functions)\n",
    "    for shingle in shingles:\n",
    "        shingle_hash = hash_shingle(shingle)\n",
    "        for i, permutation in enumerate(permutation_functions):\n",
    "            hashed_value = permutation(shingle_hash)\n",
    "            if hashed_value < signature[i]:\n",
    "                signature[i] = hashed_value\n",
    "    return signature\n",
    "\n",
    "permutation_functions = generate_permutation_function(num_permutations, max_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "16994eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_functions2 = generate_permutation_function(num_permutations2, max_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "f31ba6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_functions3 = generate_permutation_function(num_permutations3, max_hash)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "b4ccd816",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names=df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "89391ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings1 = df_names['Full Name']\n",
    "shingles1 = [generate_shingles(string, shingle_size) for string in strings1]\n",
    "#\n",
    "signatures1 = [compute_minhash_signature(shingle, permutation_functions) for shingle in shingles1]\n",
    "signatures12 = [compute_minhash_signature(shingle, permutation_functions2) for shingle in shingles1]\n",
    "signatures13 = [compute_minhash_signature(shingle, permutation_functions3) for shingle in shingles1]\n",
    "#\n",
    "df_names.insert(3, 'Signature-200', signatures1)\n",
    "signatures_at_responser = df_names['Signature-200'].to_numpy()\n",
    "signatures_at_responser = np.array([vec/np.linalg.norm(vec) for vec in signatures_at_responser])\n",
    "df_names.insert(4, 'Signature_Norm-200', signatures_at_responser.tolist())\n",
    "#\n",
    "df_names.insert(5, 'Signature-50', signatures12)\n",
    "signatures_at_responser2 = df_names['Signature-50'].to_numpy()\n",
    "signatures_at_responser2 = np.array([vec/np.linalg.norm(vec) for vec in signatures_at_responser2])\n",
    "df_names.insert(6, 'Signature_Norm-50', signatures_at_responser2.tolist())\n",
    "#\n",
    "df_names.insert(7, 'Signature-100', signatures13)\n",
    "signatures_at_responser3 = df_names['Signature-100'].to_numpy()\n",
    "signatures_at_responser3 = np.array([vec/np.linalg.norm(vec) for vec in signatures_at_responser3])\n",
    "df_names.insert(8, 'Signature_Norm-100', signatures_at_responser3.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c9e747e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fuzzy_names=df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93a379c",
   "metadata": {},
   "outputs": [],
   "source": [
    "strings1 = df_fuzzy_names['Fuzzy Full Name']\n",
    "shingles1 = [generate_shingles(string, shingle_size) for string in strings1]\n",
    "#\n",
    "signatures1 = [compute_minhash_signature(shingle, permutation_functions) for shingle in shingles1]\n",
    "signatures12 = [compute_minhash_signature(shingle, permutation_functions2) for shingle in shingles1]\n",
    "signatures13 = [compute_minhash_signature(shingle, permutation_functions3) for shingle in shingles1]\n",
    "#\n",
    "df_fuzzy_names.insert(3, 'Fuzzy Signature-200', signatures1)\n",
    "signatures_at_responser = df_fuzzy_names['Fuzzy Signature-200'].to_numpy()\n",
    "signatures_at_responser = np.array([vec/np.linalg.norm(vec) for vec in signatures_at_responser])\n",
    "df_fuzzy_names.insert(4, 'Fuzzy Signature_Norm-200', signatures_at_responser.tolist())\n",
    "#\n",
    "df_fuzzy_names.insert(5, 'Fuzzy Signature-50', signatures12)\n",
    "signatures_at_responser2 = df_fuzzy_names['Fuzzy Signature-50'].to_numpy()\n",
    "signatures_at_responser2 = np.array([vec/np.linalg.norm(vec) for vec in signatures_at_responser2])\n",
    "df_fuzzy_names.insert(6, 'Fuzzy Signature_Norm-50', signatures_at_responser2.tolist())\n",
    "#\n",
    "df_fuzzy_names.insert(7, 'Fuzzy Signature-100', signatures13)\n",
    "signatures_at_responser3 = df_fuzzy_names['Fuzzy Signature-100'].to_numpy()\n",
    "signatures_at_responser3 = np.array([vec/np.linalg.norm(vec) for vec in signatures_at_responser3])\n",
    "df_fuzzy_names.insert(8, 'Fuzzy Signature_Norm-100', signatures_at_responser3.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5fa938",
   "metadata": {},
   "outputs": [],
   "source": [
    "exact_names_filename = f\"df_names_ncvoter2014_10k_lsh200-50-100.pkl\"\n",
    "fuzzy_names_filename = f\"df_fuzzy_names_ncvoter2017_10k_lsh200-50-100.pkl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cb4626",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names.to_pickle(exact_names_filename)\n",
    "df_fuzzy_names.to_pickle(fuzzy_names_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef499237",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_fuzzy_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbcf77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88b8235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
